{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1929b2d5",
   "metadata": {},
   "source": [
    "# 1. üìö SETUP AND PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf88ba",
   "metadata": {},
   "source": [
    "## 1.1 Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8870239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccda3d68",
   "metadata": {},
   "source": [
    "## 1.2 Configuration Variables and Paths  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac12051",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_OUTLIERS_TREATED = '../dataset_output/dataset_outliers_treated.csv'\n",
    "DATASET_OUTLIERS_UNTREATED = '../dataset_output/dataset_outliers_untreated.csv'\n",
    "\n",
    "FOLDER_RESULTS = \"../results/\"\n",
    "\n",
    "FOLDER_RESULTS_EXP1 = f\"{FOLDER_RESULTS}experiment_1/\"\n",
    "FOLDER_RESULTS_EXP2 = f\"{FOLDER_RESULTS}experiment_2/\"\n",
    "FOLDER_RESULTS_EXP3 = f\"{FOLDER_RESULTS}experiment_3/\"\n",
    "FOLDER_RESULTS_EXP4 = f\"{FOLDER_RESULTS}experiment_4/\"\n",
    "\n",
    "\n",
    "RESULT_EXP1 = f\"{FOLDER_RESULTS_EXP1}experiment_1_with_smote_results.csv\"\n",
    "RESULT_EXP2= f\"{FOLDER_RESULTS_EXP2}experiment_2_without_smote_results.csv\"\n",
    "RESULT_EXP3 = f\"{FOLDER_RESULTS_EXP3}experiment_3_with_smote_results.csv\"\n",
    "RESULT_EXP4= f\"{FOLDER_RESULTS_EXP4}experiment_4_without_smote_results.csv\"\n",
    "RESULT_SUMMARY = f\"{FOLDER_RESULTS}experiments_summary.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0783892",
   "metadata": {},
   "source": [
    "## 1.3 Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02827574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    \"\"\"Load data from a CSV file\"\"\"\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"üìä Loading dataset from: {file_path}\")\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(\"-\" * 50)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee3295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_toxicity_distribution(data):\n",
    "    \"\"\"Shows the toxicity distribution with specific formatting.\"\"\"\n",
    "    print(\"-\" * 50)\n",
    "    print(\"üìä Analyzing toxicity distribution...\")\n",
    "    \n",
    "    target_counts = data['Toxicity'].value_counts()\n",
    "    target_counts.plot(kind='bar', color='orange')\n",
    "    plt.title('Original Class Distribution')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks([0, 1], ['Non-Toxic (0)', 'Toxic (1)'], rotation=0)\n",
    "    for i, v in enumerate(target_counts):\n",
    "        plt.text(i, v + 1, str(v), ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Original distribution:\")\n",
    "    print(\"Non-Toxic (0):\", (data['Toxicity'] == 0).sum())\n",
    "    print(\"Toxic (1):\", (data['Toxicity'] == 1).sum())\n",
    "    print(\"\\nProportions:\")\n",
    "    proportions = data['Toxicity'].value_counts(normalize=True)\n",
    "    print(f\"Non-Toxic (0): {proportions[0]:.3f} ({proportions[0]*100:.1f}%)\")\n",
    "    print(f\"Toxic (1): {proportions[1]:.3f} ({proportions[1]*100:.1f}%)\")\n",
    "    \n",
    "    print(\"‚úÖ Distribution analysis completed!\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b59276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_split(data, target_column='Toxicity', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepares and splits the data into training and test sets.\n",
    "    \"\"\"\n",
    "    print(\"-\" * 50)\n",
    "    print(\"üîÑ Preparing data split...\")\n",
    "    \n",
    "    X = data.drop(columns=target_column)\n",
    "    y = data[target_column]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüîπ Total samples: {len(X)}\")\n",
    "    print(f\"üîπ Training: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"üîπ Test: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüîπ Class distribution:\")\n",
    "    \n",
    "    total_0 = (y == 0).sum()\n",
    "    total_1 = (y == 1).sum()\n",
    "    print(f\"Total - Class 0: {total_0} ({total_0/len(y)*100:.1f}%), Class 1: {total_1} ({total_1/len(y)*100:.1f}%)\")\n",
    "    \n",
    "    train_0 = (y_train == 0).sum()\n",
    "    train_1 = (y_train == 1).sum()\n",
    "    print(f\"Train - Class 0: {train_0} ({train_0/len(y_train)*100:.1f}%), Class 1: {train_1} ({train_1/len(y_train)*100:.1f}%)\")\n",
    "    \n",
    "    test_0 = (y_test == 0).sum()\n",
    "    test_1 = (y_test == 1).sum()\n",
    "    print(f\"Test  - Class 0: {test_0} ({test_0/len(y_test)*100:.1f}%), Class 1: {test_1} ({test_1/len(y_test)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"‚úÖ Data split completed!\")\n",
    "    print(\"-\" * 50)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5a5eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote_balancing(X_train, y_train, use_smote=True, random_state=42):\n",
    "    \"\"\"\n",
    "    Applies SMOTE to balance classes in the training set or returns original data.\n",
    "    \"\"\"\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if use_smote:\n",
    "        print(\"‚öñÔ∏è Applying SMOTE balancing...\")\n",
    "    else:\n",
    "        print(\"‚öñÔ∏è Using original imbalanced data (no SMOTE)...\")\n",
    "    \n",
    "    original_0 = (y_train == 0).sum()\n",
    "    original_1 = (y_train == 1).sum()\n",
    "    print(f\"\\nüîπ Original distribution:\")\n",
    "    print(f\"Class 0: {original_0} ({original_0/len(y_train)*100:.1f}%)\")\n",
    "    print(f\"Class 1: {original_1} ({original_1/len(y_train)*100:.1f}%)\")\n",
    "    \n",
    "    if use_smote:\n",
    "        smote = SMOTE(random_state=random_state)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        new_0 = (y_train_res == 0).sum()\n",
    "        new_1 = (y_train_res == 1).sum()\n",
    "        print(f\"\\nüîπ After SMOTE distribution:\")\n",
    "        print(f\"Class 0: {new_0} ({new_0/len(y_train_res)*100:.1f}%)\")\n",
    "        print(f\"Class 1: {new_1} ({new_1/len(y_train_res)*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nüîπ CHANGES:\")\n",
    "        print(f\"Samples added: {len(y_train_res) - len(y_train)}\")\n",
    "        print(f\"Minority class increased: {new_1 - original_1} samples\")\n",
    "        \n",
    "        print(\"‚úÖ SMOTE balancing completed!\")\n",
    "    else:\n",
    "        X_train_res, y_train_res = X_train, y_train\n",
    "        \n",
    "        print(f\"\\nüîπ Final distribution (unchanged):\")\n",
    "        print(f\"Class 0: {original_0} ({original_0/len(y_train_res)*100:.1f}%)\")\n",
    "        print(f\"Class 1: {original_1} ({original_1/len(y_train_res)*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nüîπ NO CHANGES:\")\n",
    "        print(f\"Total samples: {len(y_train_res)}\")\n",
    "        print(f\"Data remains imbalanced\")\n",
    "        \n",
    "        print(\"‚úÖ Original data ready for training!\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    return X_train_res, y_train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0f00a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_and_params():\n",
    "    \"\"\"\n",
    "    Define los modelos y sus par√°metros para optimizaci√≥n.\n",
    "    \"\"\"\n",
    "    models_params = {\n",
    "        \"Logistic Regression\": {\n",
    "            'model': LogisticRegression(max_iter=1000, random_state=42),\n",
    "            'params': {\n",
    "                'C': [0.1, 1.0, 10.0],\n",
    "                'solver': ['liblinear', 'lbfgs']\n",
    "            }\n",
    "        },\n",
    "        \"Decision Tree\": {\n",
    "            'model': DecisionTreeClassifier(random_state=42),\n",
    "            'params': {\n",
    "                'max_depth': [3, 5, 7, 10],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            }\n",
    "        },\n",
    "        \"Random Forest\": {\n",
    "            'model': RandomForestClassifier(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'min_samples_split': [2, 5],\n",
    "                'min_samples_leaf': [1, 2]\n",
    "            }\n",
    "        },\n",
    "        \"XGBoost\": {\n",
    "            'model': xgb.XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [3, 4, 5],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'subsample': [0.8, 1.0]\n",
    "            }\n",
    "        },\n",
    "        \"LightGBM\": {\n",
    "            'model': lgb.LGBMClassifier(verbosity=-1, random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [3, 4, 5],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'num_leaves': [20, 31, 50]\n",
    "            }\n",
    "        },\n",
    "        \"SVM\": {\n",
    "            'model': make_pipeline(StandardScaler(), SVC(kernel='rbf', probability=True, random_state=42)),\n",
    "            'params': {\n",
    "                'svc__C': [0.1, 1.0, 10.0],\n",
    "                'svc__gamma': ['scale', 'auto', 0.01, 0.1]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return models_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08156ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_optimize_models(X_train, y_train, X_test, y_test, cv_folds=5, experiment_name=\"Experiment\"):\n",
    "    \"\"\"\n",
    "    Trains and optimizes multiple models using GridSearchCV and cross-validation.\n",
    "    Includes classic classification metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"ü§ñ TRAINING MODELS - {experiment_name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    models_params = get_models_and_params()\n",
    "    results = []\n",
    "    best_models = {}\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    for model_name, config in models_params.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"üîß Training: {model_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        model = config['model']\n",
    "        param_grid = config['params']\n",
    "        \n",
    "        print(\"üîç Performing hyperparameter optimization...\")\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            cv=skf,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_models[model_name] = best_model\n",
    "        \n",
    "        y_pred_train = best_model.predict(X_train)\n",
    "        y_pred_test = best_model.predict(X_test)\n",
    "        y_pred_proba_train = best_model.predict_proba(X_train)[:, 1]\n",
    "        y_pred_proba_test = best_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        cv_scores = cross_val_score(best_model, X_train, y_train, cv=skf, scoring='roc_auc')\n",
    "        \n",
    "        train_auc = roc_auc_score(y_train, y_pred_proba_train)\n",
    "        test_auc = roc_auc_score(y_test, y_pred_proba_test)\n",
    "        \n",
    "        train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "        train_precision = precision_score(y_train, y_pred_train, average='weighted')\n",
    "        train_recall = recall_score(y_train, y_pred_train, average='weighted')\n",
    "        train_f1 = f1_score(y_train, y_pred_train, average='weighted')\n",
    "        \n",
    "        test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "        test_precision = precision_score(y_test, y_pred_test, average='weighted')\n",
    "        test_recall = recall_score(y_test, y_pred_test, average='weighted')\n",
    "        test_f1 = f1_score(y_test, y_pred_test, average='weighted')\n",
    "        \n",
    "        result = {\n",
    "            'Model': model_name,\n",
    "            'Best_Params': str(grid_search.best_params_),\n",
    "            'CV_AUC_Mean': cv_scores.mean(),\n",
    "            'CV_AUC_Std': cv_scores.std(),\n",
    "            'Best_CV_Score': grid_search.best_score_,\n",
    "            'Train_AUC': train_auc,\n",
    "            'Train_Accuracy': train_accuracy,\n",
    "            'Train_Precision': train_precision,\n",
    "            'Train_Recall': train_recall,\n",
    "            'Train_F1': train_f1,\n",
    "            'Test_AUC': test_auc,\n",
    "            'Test_Accuracy': test_accuracy,\n",
    "            'Test_Precision': test_precision,\n",
    "            'Test_Recall': test_recall,\n",
    "            'Test_F1': test_f1\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"‚úÖ Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"üìä Cross-validation AUC: {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})\")\n",
    "        print(f\"üìä Best CV Score: {grid_search.best_score_:.3f}\")\n",
    "        \n",
    "        print(f\"\\nüìà TRAINING METRICS:\")\n",
    "        print(f\"   AUC: {train_auc:.3f}\")\n",
    "        print(f\"   Accuracy: {train_accuracy:.3f}\")\n",
    "        print(f\"   Precision: {train_precision:.3f}\")\n",
    "        print(f\"   Recall: {train_recall:.3f}\")\n",
    "        print(f\"   F1-Score: {train_f1:.3f}\")\n",
    "        \n",
    "        print(f\"\\nüéØ TEST METRICS:\")\n",
    "        print(f\"   AUC: {test_auc:.3f}\")\n",
    "        print(f\"   Accuracy: {test_accuracy:.3f}\")\n",
    "        print(f\"   Precision: {test_precision:.3f}\")\n",
    "        print(f\"   Recall: {test_recall:.3f}\")\n",
    "        print(f\"   F1-Score: {test_f1:.3f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('Test_AUC', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üèÜ FINAL RESULTS - {experiment_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    display_cols = ['Model', 'CV_AUC_Mean', 'Test_AUC', 'Test_Accuracy', 'Test_F1']\n",
    "    print(results_df[display_cols].to_string(index=False))\n",
    "    \n",
    "    return results_df, best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a043f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_detailed_results(results_df, experiment_name):\n",
    "    \"\"\"\n",
    "    Shows a detailed summary of all metrics by model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìä DETAILED METRICS SUMMARY - {experiment_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for idx, row in results_df.iterrows():\n",
    "        print(f\"\\nüî∏ {row['Model']} (Rank #{idx+1})\")\n",
    "        print(f\"   Cross-Validation AUC: {row['CV_AUC_Mean']:.3f} (¬±{row['CV_AUC_Std']:.3f})\")\n",
    "        print(f\"   Test Metrics:\")\n",
    "        print(f\"      AUC:       {row['Test_AUC']:.3f}\")\n",
    "        print(f\"      Accuracy:  {row['Test_Accuracy']:.3f}\")\n",
    "        print(f\"      Precision: {row['Test_Precision']:.3f}\")\n",
    "        print(f\"      Recall:    {row['Test_Recall']:.3f}\")\n",
    "        print(f\"      F1-Score:  {row['Test_F1']:.3f}\")\n",
    "        print(f\"   Best Params: {row['Best_Params']}\")\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541969b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_roc_curve(y_test, y_pred_proba, model_name, figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Generates only the ROC Curve plot.\n",
    "    \"\"\"\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    ax.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "            label=f'ROC curve (AUC = {auc_score:.3f})')\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "            label='Random classifier')\n",
    "    \n",
    "    ax.set_xlabel('False Positive Rate', fontsize=16)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=16)\n",
    "    ax.set_title(f'ROC Curve - {model_name}\\nAUC Score: {auc_score:.3f}', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5677aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confusion_matrix(model, X_test, y_test, model_name, figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Generates only the Confusion Matrix plot.\n",
    "    \"\"\"\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ConfusionMatrixDisplay.from_estimator(\n",
    "        model, X_test, y_test, \n",
    "        cmap='Blues',\n",
    "        ax=ax,\n",
    "        colorbar=True,\n",
    "        display_labels=['Non-Toxic (0)', 'Toxic (1)']\n",
    "    )\n",
    "    \n",
    "    plt.title(f\"Confusion Matrix - {model_name}\\nAUC Score: {auc_score:.3f}\", \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54b2d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_model_evaluation(model, model_name, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Detailed evaluation of a specific model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üìä DETAILED EVALUATION: {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"üéØ AUC Score: {auc_score:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüìã Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(f\"\\nüî¢ Confusion Matrix:\")\n",
    "    generate_confusion_matrix(model, X_test, y_test, model_name)\n",
    "    \n",
    "    print(f\"\\nüìà ROC Curve:\")\n",
    "    generate_roc_curve(y_test, y_pred_proba, model_name)\n",
    "    \n",
    "    return auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c36a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(model, model_name, experiment_name, save_path=\"../results/\"):\n",
    "    \"\"\"\n",
    "    Saves only the trained model.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüíæ SAVING MODEL: {model_name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        exp_clean = experiment_name.replace(\" \", \"_\").replace(\":\", \"\").replace(\"(\", \"\").replace(\")\", \"\").lower()\n",
    "        model_clean = model_name.replace(\" \", \"_\").lower()\n",
    "        \n",
    "        model_filename = f\"{exp_clean}_{model_clean}_{timestamp}.pkl\"\n",
    "        model_path = os.path.join(save_path, model_filename)\n",
    "        \n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        \n",
    "        print(f\"‚úÖ Model saved successfully!\")\n",
    "        print(f\"üìÅ File: {model_path}\")\n",
    "        \n",
    "        return model_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving model: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13949484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_best_model(results_df, models_dict, X_test, y_test, experiment_name, save_model=True, save_path=\"../results/\"):\n",
    "    \"\"\"\n",
    "    Evaluates the best model from a specific experiment and optionally saves it.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üîç DETAILED EVALUATION - BEST MODEL FROM {experiment_name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    best_model_name = results_df.iloc[0]['Model']\n",
    "    best_model = models_dict[best_model_name]\n",
    "    best_auc = results_df.iloc[0]['Test_AUC']\n",
    "    best_cv_auc = results_df.iloc[0]['CV_AUC_Mean']\n",
    "    best_cv_std = results_df.iloc[0]['CV_AUC_Std']\n",
    "    \n",
    "    print(f\"üèÜ Best Model: {best_model_name}\")\n",
    "    print(f\"üéØ Test AUC: {best_auc:.3f}\")\n",
    "    print(f\"üìä CV AUC: {best_cv_auc:.3f} (¬±{best_cv_std:.3f})\")\n",
    "    print(f\"üìã Best Parameters: {results_df.iloc[0]['Best_Params']}\")\n",
    "    \n",
    "    auc_score = detailed_model_evaluation(best_model, best_model_name, X_test, y_test)\n",
    "    \n",
    "    model_path = None\n",
    "    if save_model:\n",
    "        model_path = save_best_model(best_model, best_model_name, experiment_name, save_path)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Best model evaluation for {experiment_name} completed!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return best_model_name, best_model, auc_score, model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9309391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_experiments_analysis(results_exp1, results_exp2, results_exp3, results_exp4):\n",
    "    \"\"\"\n",
    "    Simple analysis of all 4 experiments.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"üèÅ FINAL EXPERIMENTS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    experiments = [\n",
    "        (\"Experiment 1: Outliers treated with SMOTE\", results_exp1.iloc[0]),\n",
    "        (\"Experiment 2: Outliers treated without SMOTE\", results_exp2.iloc[0]),\n",
    "        (\"Experiment 3: Outliers untreated with SMOTE\", results_exp3.iloc[0]),\n",
    "        (\"Experiment 4: Outliers untreated without SMOTE\", results_exp4.iloc[0])\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüèÜ BEST MODELS BY EXPERIMENT:\")\n",
    "    print(\"-\" * 70)\n",
    "    for exp_name, best_model in experiments:\n",
    "        print(f\"{exp_name}\")\n",
    "        print(f\"   Model: {best_model['Model']}\")\n",
    "        print(f\"   Test AUC: {best_model['Test_AUC']:.3f}\")\n",
    "        print(f\"   Accuracy: {best_model['Test_Accuracy']:.3f}\")\n",
    "        print(f\"   F1-Score: {best_model['Test_F1']:.3f}\")\n",
    "        print()\n",
    "    \n",
    "    best_auc = max(exp[1]['Test_AUC'] for exp in experiments)\n",
    "    winner = next(exp for exp in experiments if exp[1]['Test_AUC'] == best_auc)\n",
    "    \n",
    "    print(\"ü•á WINNING EXPERIMENT:\")\n",
    "    print(f\"   {winner[0]}\")\n",
    "    print(f\"   Model: {winner[1]['Model']}\")\n",
    "    print(f\"   Test AUC: {winner[1]['Test_AUC']:.3f}\")\n",
    "    print()\n",
    "    \n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5407a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_simple(results_exp1, results_exp2, results_exp3, results_exp4):\n",
    "    \"\"\"\n",
    "    Save results to CSV files.\n",
    "    \"\"\"\n",
    "    print(\"üíæ SAVING RESULTS...\")\n",
    "    \n",
    "    results_exp1.to_csv(RESULT_EXP1, index=False)\n",
    "    results_exp2.to_csv(RESULT_EXP2, index=False) \n",
    "    results_exp3.to_csv(RESULT_EXP3, index=False)\n",
    "    results_exp4.to_csv(RESULT_EXP4, index=False)\n",
    "    \n",
    "    summary_data = [\n",
    "        {\n",
    "            'Experiment': 'EXP1_WITH_OUTLIERS_TREATED_WITH_SMOTE',\n",
    "            'Best_Model': results_exp1.iloc[0]['Model'],\n",
    "            'Test_AUC': results_exp1.iloc[0]['Test_AUC'],\n",
    "            'Test_Accuracy': results_exp1.iloc[0]['Test_Accuracy'],\n",
    "            'Test_F1': results_exp1.iloc[0]['Test_F1']\n",
    "        },\n",
    "        {\n",
    "            'Experiment': 'EXP2_WITH_OUTLIERS_TREATED_NO_SMOTE',\n",
    "            'Best_Model': results_exp2.iloc[0]['Model'],\n",
    "            'Test_AUC': results_exp2.iloc[0]['Test_AUC'],\n",
    "            'Test_Accuracy': results_exp2.iloc[0]['Test_Accuracy'],\n",
    "            'Test_F1': results_exp2.iloc[0]['Test_F1']\n",
    "        },\n",
    "        {\n",
    "            'Experiment': 'EXP3_WITH_OUTLIERS_UNTREATED_WITH_SMOTE',\n",
    "            'Best_Model': results_exp3.iloc[0]['Model'],\n",
    "            'Test_AUC': results_exp3.iloc[0]['Test_AUC'],\n",
    "            'Test_Accuracy': results_exp3.iloc[0]['Test_Accuracy'],\n",
    "            'Test_F1': results_exp3.iloc[0]['Test_F1']\n",
    "        },\n",
    "        {\n",
    "            'Experiment': 'EXP4_WITH_OUTLIERS_UNTREATED_NO_SMOTE',\n",
    "            'Best_Model': results_exp4.iloc[0]['Model'],\n",
    "            'Test_AUC': results_exp4.iloc[0]['Test_AUC'],\n",
    "            'Test_Accuracy': results_exp4.iloc[0]['Test_Accuracy'],\n",
    "            'Test_F1': results_exp4.iloc[0]['Test_F1']\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv(RESULT_SUMMARY, index=False)\n",
    "    \n",
    "    print(\"‚úÖ Results saved:\")\n",
    "    print(f\"   üìÅ {RESULT_EXP1}\")\n",
    "    print(f\"   üìÅ {RESULT_EXP2}\")\n",
    "    print(f\"   üìÅ {RESULT_EXP3}\")\n",
    "    print(f\"   üìÅ {RESULT_EXP4}\")\n",
    "    print(f\"   üìÅ {RESULT_SUMMARY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f22e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_best_models_roc_curves(figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Compare ROC curves of the best models from all 4 experiments in a single plot.\n",
    "    \"\"\"\n",
    "    print(\"üìä ROC CURVES COMPARISON - BEST MODELS FROM EACH EXPERIMENT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    experiments_data = [\n",
    "        (\"Experiment 1: Outliers treated with SMOTE\", best_model_exp1, X_test_1, y_test_1, best_model_name_exp1),\n",
    "        (\"Experiment 2: Outliers treated without SMOTE\", best_model_exp2, X_test_2, y_test_2, best_model_name_exp2),\n",
    "        (\"Experiment 3: Outliers untreated with SMOTE\", best_model_exp3, X_test_3, y_test_3, best_model_name_exp3),\n",
    "        (\"Experiment 4: Outliers untreated without SMOTE\", best_model_exp4, X_test_4, y_test_4, best_model_name_exp4)\n",
    "    ]\n",
    "    \n",
    "    colors = ['darkorange', 'green', 'red', 'blue']\n",
    "    \n",
    "    for i, (exp_name, model, X_test, y_test, model_name) in enumerate(experiments_data):\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        plt.plot(fpr, tpr, color=colors[i], lw=2,\n",
    "                label=f'{exp_name}\\n{model_name} (AUC = {auc_score:.3f})')\n",
    "        \n",
    "        print(f\"üìà {exp_name}\")\n",
    "        print(f\"   AUC: {auc_score:.3f}\")\n",
    "        print()\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "             label='Random classifier (AUC = 0.500)')\n",
    "    \n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.title('ROC Curves Comparison - Best Models by Experiment', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.legend(loc=\"lower right\", fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ ROC curves comparison completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad86dfc",
   "metadata": {},
   "source": [
    "# 2. üóÇÔ∏è DATA LOADING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a9e56",
   "metadata": {},
   "source": [
    "## 2.1 Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd33e4d7",
   "metadata": {},
   "source": [
    "### 2.1.1 Dataset with Treated Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc2dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_outliers_treated = load_dataset(DATASET_OUTLIERS_TREATED)\n",
    "show_toxicity_distribution(data_outliers_treated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bec4df",
   "metadata": {},
   "source": [
    "### 2.1.2 Dataset with Untreated Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2d4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_outliers_untreated = load_dataset(DATASET_OUTLIERS_UNTREATED)\n",
    "show_toxicity_distribution(data_outliers_untreated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77956125",
   "metadata": {},
   "source": [
    "# 3. üß™ EXPERIMENTAL DESIGN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ce060d",
   "metadata": {},
   "source": [
    "## 3.1 Experimental Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a416a1",
   "metadata": {},
   "source": [
    "- **4 Systematic Experiments** evaluating combinations of:\n",
    "  - Outlier treatment: ‚úÖ Treated vs ‚ùå Untreated\n",
    "  - Class balancing: ‚úÖ SMOTE vs ‚ùå Original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af963bb8",
   "metadata": {},
   "source": [
    "## 3.2 Experiment Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88788354",
   "metadata": {},
   "source": [
    "| Experiment | Outliers | SMOTE | Description |\n",
    "|------------|----------|-------|-------------|\n",
    "| EXP1 | ‚úÖ Treated | ‚úÖ Applied | Outliers treated with SMOTE |\n",
    "| EXP2 | ‚úÖ Treated | ‚ùå Not applied | Outliers treated without SMOTE |\n",
    "| EXP3 | ‚ùå Untreated | ‚úÖ Applied | Outliers untreated with SMOTE |\n",
    "| EXP4 | ‚ùå Untreated | ‚ùå Not applied | Outliers untreated without SMOTE |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0887ce5",
   "metadata": {},
   "source": [
    "# 4. üöÄ EXPERIMENTS EXECUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296eac0",
   "metadata": {},
   "source": [
    "## 4.1 Experiment 1: Outliers treated with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbf6bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üöÄ Experiment 1: Outliers treated with SMOTE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e81c51",
   "metadata": {},
   "source": [
    "### 4.1.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cd6665",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_test_1, y_train_1, y_test_1 = prepare_data_split(data=data_outliers_treated, target_column='Toxicity', test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb3892",
   "metadata": {},
   "source": [
    "### 4.1.2 SMOTE Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e4af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_balanced, y_train_balanced = apply_smote_balancing(X_train_1, y_train_1, use_smote=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519c2f9",
   "metadata": {},
   "source": [
    "### 4.1.3 Training and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb7c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_exp1, models_exp1 = train_and_optimize_models(X_train_balanced, y_train_balanced, X_test_1, y_test_1, cv_folds=5, experiment_name=\"Outliers treated with SMOTE\")\n",
    "\n",
    "display_detailed_results(results_exp1, \"Outliers treated with SMOTE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3c5310",
   "metadata": {},
   "source": [
    "### 4.1.4 Best Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd87c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name_exp1, best_model_exp1, auc_exp1, model_path_exp1 = evaluate_best_model(results_exp1, models_exp1, X_test_1, y_test_1, \"Experiment 1\", save_model=True, save_path=FOLDER_RESULTS_EXP1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f158f26",
   "metadata": {},
   "source": [
    "## 4.2 Experiment 2: Outliers treated without SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b4c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üöÄ Experiment 2: Outliers treated without SMOTE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e54ba25",
   "metadata": {},
   "source": [
    "### 4.2.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d29f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = prepare_data_split(data=data_outliers_treated, target_column='Toxicity', test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df73ba65",
   "metadata": {},
   "source": [
    "### 4.2.2 Original Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57144de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_original, y_train_original = apply_smote_balancing(X_train_2, y_train_2, use_smote=False, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddc9968",
   "metadata": {},
   "source": [
    "### 4.2.3 Training and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36695a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_exp2, models_exp2 = train_and_optimize_models(X_train_original, y_train_original, X_test_2, y_test_2, cv_folds=5, experiment_name=\"Outliers treated without SMOTE\")\n",
    "\n",
    "display_detailed_results(results_exp2, \"Outliers treated without SMOTE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b995c627",
   "metadata": {},
   "source": [
    "### 4.2.4 Best Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9d9d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name_exp2, best_model_exp2, auc_exp2, model_path_exp2 = evaluate_best_model(results_exp2, models_exp2, X_test_2, y_test_2, \"Experiment 2\", save_model=True, save_path=FOLDER_RESULTS_EXP2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bdbb5a",
   "metadata": {},
   "source": [
    "## 4.3 Experiment 3: Outliers untreated with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e29ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üöÄ Experiment 3: Outliers untreated with SMOTE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c5d82d",
   "metadata": {},
   "source": [
    "### 4.3.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed9a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3, X_test_3, y_train_3, y_test_3 = prepare_data_split(data=data_outliers_untreated, target_column='Toxicity', test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291cc90d",
   "metadata": {},
   "source": [
    "### 4.3.2 SMOTE Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a48ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_balanced_3, y_train_balanced_3 = apply_smote_balancing(X_train_3, y_train_3, use_smote=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68bf8ff",
   "metadata": {},
   "source": [
    "### 4.3.3 Training and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc5dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_exp3, models_exp3 = train_and_optimize_models(X_train_balanced_3, y_train_balanced_3, X_test_3, y_test_3, cv_folds=5, experiment_name=\"Outliers untreated with SMOTE\")\n",
    "\n",
    "display_detailed_results(results_exp3, \"Outliers untreated with SMOTE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4915fdbc",
   "metadata": {},
   "source": [
    "### 4.3.4 Best Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4575ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name_exp3, best_model_exp3, auc_exp3, model_path_exp3 = evaluate_best_model(results_exp3, models_exp3, X_test_3, y_test_3, \"Experiment 3\", save_model=True, save_path=FOLDER_RESULTS_EXP3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b642d02c",
   "metadata": {},
   "source": [
    "## 4.4 Experiment 4: Outliers untreated without SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7df65ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üöÄ Experiment 4: Outliers untreated without SMOTE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cfa0ec",
   "metadata": {},
   "source": [
    "### 4.4.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_4, X_test_4, y_train_4, y_test_4 = prepare_data_split(data=data_outliers_untreated, target_column='Toxicity', test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da843b70",
   "metadata": {},
   "source": [
    "### 4.4.2 Original Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b601726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_original_4, y_train_original_4 = apply_smote_balancing(X_train_4, y_train_4, use_smote=False, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8fb78e",
   "metadata": {},
   "source": [
    "### 4.4.3 Training and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212bf312",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_exp4, models_exp4 = train_and_optimize_models(\n",
    "    X_train_original_4, y_train_original_4, X_test_4, y_test_4, \n",
    "    cv_folds=5, experiment_name=\"Outliers untreated without SMOTE\"\n",
    ")\n",
    "\n",
    "display_detailed_results(results_exp4, \"Outliers untreated without SMOTE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae7c2a",
   "metadata": {},
   "source": [
    "### 4.4.4 Best Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231bc1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name_exp4, best_model_exp4, auc_exp4, model_path_exp4 = evaluate_best_model(results_exp4, models_exp4, X_test_4, y_test_4, \"Experiment 4\", save_model=True, save_path=FOLDER_RESULTS_EXP4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db4cd68",
   "metadata": {},
   "source": [
    "# 5. üìä COMPARATIVE ANALYSIS AND RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f5258",
   "metadata": {},
   "source": [
    "## 5.1 Comprehensive Experiments Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf8b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "winner = simple_experiments_analysis(results_exp1, results_exp2, results_exp3, results_exp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8966b7e",
   "metadata": {},
   "source": [
    "## 5.2 ROC Curves Comparative Visualization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77dcc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_best_models_roc_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19549369",
   "metadata": {},
   "source": [
    "## 5.2 Results Export and Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e297cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results_simple(results_exp1, results_exp2, results_exp3, results_exp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4315b7",
   "metadata": {},
   "source": [
    "## 5.3 Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d0e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéâ ANALYSIS COMPLETED!\")\n",
    "print(f\"Winner: {winner[0]}\")\n",
    "print(f\"Best model: {winner[1]['Model']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanotox-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
